{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<style>\n",
    "h1 {\n",
    "    position: sticky;\n",
    "}\n",
    "h2, h3 {\n",
    "    position: sticky;\n",
    "    top: 0;\n",
    "    background-color: white;\n",
    "    z-index: 1000;\n",
    "    padding: 10px;\n",
    "    box-shadow: 1px 2px 5px rgba(0, 0, 0, 0);\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"RNN - Recurrent Neural Network\"\n",
    "subtitle: \"Bending Time with RNNs - How AI remembers\"\n",
    "title-block-banner: \"linear-gradient(43deg, #0a26ad 0%, #c468e0 46%,rgb(236, 113, 113) 100%)\"\n",
    "embed-resources: true\n",
    "\n",
    "format: \n",
    "   \n",
    "    html: \n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        toc-depth: 5\n",
    "        number-sections: false\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "        favicon: \"1.png\"\n",
    "        include-in-header: include.html\n",
    "        code-line-numbers: true\n",
    "        theme: flatly\n",
    "        mermaid: \n",
    "            theme: default\n",
    "            align: center\n",
    "        grid:\n",
    "            sidebar-width: 400px\n",
    "            body-width: 1300px\n",
    "            margin-width: 1px\n",
    "            gutter-width: 1.5rem\n",
    "        \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://ai4sme.aisingapore.org/wp-content/uploads/2022/06/animated1.gif' width = 700><br> RNN</img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN stands for Recurrent Neural Network, which is a type of artificial neural network that processes sequential data. RNNs are used in a variety of applications, such as speech recognition and sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why ANN can't be used in sequential data?\n",
    "Artificial Neural Networks (ANNs) struggle with sequential data due to inherent structural limitations. Here's a breakdown of the key reasons:\n",
    "\n",
    "\n",
    "##### **Reason 1. Fixed Input/Output Size Requirement**\n",
    "In real life, sequential data (text, time series, sensor readings) often has variable lengths. For example:\n",
    " e.g., \n",
    "    <table>\n",
    "    <tr>\n",
    "    <th> Sequence</th> <th>Size</th></tr>\n",
    "    <tr>\n",
    "    <td> Python is a OOPS programming language </td> <td> 6 </td> </tr>\n",
    "    <tr><td> I Love India </td><td>3</td></tr>\n",
    "    <tr><td> I am playing football </td><td>4</td></tr>\n",
    "    </table>\n",
    "\n",
    "\n",
    "* Suppose you make an ANN having the below structure.\n",
    "* It has 3 input nodes.\n",
    "\n",
    "\n",
    "   <img src = 'https://lh3.googleusercontent.com/d/13sdaMqTjeJvpar38DRKHi_FPU8gDxZIB' width=500>\n",
    "\n",
    "\n",
    "* Our first sentence contains 6 words, hence the weight metrics will be 6 * 5 structure.\n",
    "* The second sentence contains 3 words hence the weight metrics will be 3 * 5 structure.\n",
    "* The third sentence contains 4 words hence the weight metrics will be 4 * 5 structure.\n",
    "\n",
    "We can see here the structure of the input weight metrics is changing based on the input text, which is not practical for designing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 2. Zero Padding Unnecessary Computation**\n",
    "\n",
    "* To solve the first issue of varying length we can use the zero padding technique.\n",
    "* First, we can count the sentence having maximum words.\n",
    "* In our case we have the first sentence having a maximum of 6 number of words.\n",
    "* So we will fix our input text size to a maximum of 6 words.\n",
    "* In the second sentence, we have a number of words, as we have fixed our input to  6 words, but we have 3 words in 2nd sentence hence we will append 3 more vectors having zero values inside it.\n",
    "* Hence it is called **zero padding**.\n",
    "\n",
    "    <img src='https://lh3.googleusercontent.com/d/1MWF7loDs7ICUG4LEXeUCvPlbpuHqr5Ry'></img>\n",
    "\n",
    "\n",
    "* The problem with zero padding is that if we have the maximum word of a sentence is 1000 words.\n",
    "* Then we will fix the input length to 1000 nodes.\n",
    "* But if we got a sentence having only 5 words then for the rest of the 995 words we have to use zero padding.\n",
    "\n",
    "Which will take extra memory and computation power, decrease the training speed of the model and  undesirable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 3. Prediction Problem On Different Input Length**\n",
    "\n",
    "* In our case, we have set our input length to 6 words while training the model.\n",
    "* But while predicting suppose we got an input text having the length of 10 words, at that time our model will fail.\n",
    "* Because we have trained our model with a fixed input size of 6 words, it will not be able to predict for 10 words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 4. Not Considering Sequential Information**\n",
    "\n",
    "* ANN architecture does not take into account the sequence information of the input text.\n",
    "* When we pass the input text to the ANN model it will take all the input at a time.\n",
    "* When we enter vales at a time it will be mixed up inside the network, hence the sequence information is discarded.\n",
    "* The sequence information is discarded in the ANN model.\n",
    "* Hence it is not suitable for the sequential data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 5. Lack of temporal memory**\n",
    "\n",
    "ANNs process inputs independently, with no mechanism to retain information from previous steps. This makes them unsuitable for tasks requiring context, such as:\n",
    "\n",
    "- Language: The word “lie” means different things in “never tell a lie” vs. “lie down”.\n",
    "\n",
    "- Time series: Predicting stock prices requires historical trends, not just isolated data points.\n",
    "\n",
    "Example: In the sentence “The cat chased the…”, ANNs cannot retain the context of “cat” to predict “mouse” as the next word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN Forward Propagation - Step by Step\n",
    "\n",
    "In forward propagation of an RNN (Recurrent Neural Network), the network processes input sequences step by step. At each time step $t$, it takes the current input $x_t$ and the hidden state from the previous step $h_{t−1}$, applies weights and activation functions (like tanh), and computes the new hidden state $h_t$. This hidden state is then used to predict the output $y_t$. \n",
    "\n",
    "The process repeats for each time step, allowing the RNN to capture temporal dependencies in sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1w2z832C_B6xDovwm7ypJg.jpeg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Problem Setup**\n",
    "We have a dataset of sentences:\n",
    "\n",
    "| Sequence | Size |\n",
    "|----------|------|\n",
    "| \"Show is nice\" | 3 |\n",
    "| \"Show is not nice\" | 4 |\n",
    "| \"Show is worst\" | 3 |\n",
    "\n",
    "Each word is represented using **One-Hot Encoding (OHE)**.\n",
    "\n",
    "### **2. Network Architecture**\n",
    "\n",
    "- **Input Layer:** 5 neurons (each representing a one-hot encoded word)\n",
    "- **Hidden Layer:** 3 neurons (processing sequential information)\n",
    "- **Output Layer:** 1 neuron (final prediction using softmax activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {\"flowchart\": {\"nodeSpacing\": 20, \"rankSpacing\": 40, 'height':1}}}%%\n",
    "graph LR\n",
    "    subgraph Inputs\n",
    "        direction LR\n",
    "        style Inputs fill:#a7bde0,stroke:#64b5f6,font-size:30,stroke-width:2px\n",
    "        x1[x<sub>1</sub>]\n",
    "        x2[x<sub>2</sub>]\n",
    "        x3[x<sub>3</sub>]\n",
    "        x4[x<sub>4</sub>]\n",
    "        x5[x<sub>5</sub>]\n",
    "    end\n",
    "    \n",
    "    subgraph \"hidden-layer\" [\"Hidden Layer\"]\n",
    "        direction LR\n",
    "        style hidden-layer fill:#a7e0b3,stroke:#85ff9f,font-size:30,stroke-width:2px\n",
    "        h1(h<sub>1</sub>)\n",
    "        h2(h<sub>2</sub>)\n",
    "        h3(h<sub>3</sub>)\n",
    "    end\n",
    "    \n",
    "    subgraph Output\n",
    "        direction LR\n",
    "        style Output fill:#e78383,stroke:#f8cc52,font-size:30,stroke-width:2px\n",
    "        y(y<sub>1</sub>)\n",
    "    end\n",
    "\n",
    "    x1 --> |w<sub>11</sub><sup>1</sup>| h1\n",
    "    x1 --> |w<sub>12</sub><sup>1</sup>| h2\n",
    "    x1 --> |w<sub>13</sub><sup>1</sup>| h3\n",
    "    x2 --> |w<sub>21</sub><sup>1</sup>| h1\n",
    "    x2 --> |w<sub>22</sub><sup>1</sup>| h2\n",
    "    x2 --> |w<sub>23</sub><sup>1</sup>| h3\n",
    "    x3 --> |w<sub>31</sub><sup>1</sup>| h1\n",
    "    x3 --> |w<sub>32</sub><sup>1</sup>| h2\n",
    "    x3 --> |w<sub>33</sub><sup>1</sup>| h3\n",
    "    x4 --> |w<sub>41</sub><sup>1</sup>| h1\n",
    "    x4 --> |w<sub>42</sub><sup>1</sup>| h2\n",
    "    x4 --> |w<sub>43</sub><sup>1</sup>| h3\n",
    "    x5 --> |w<sub>51</sub><sup>1</sup>| h1\n",
    "    x5 --> |w<sub>52</sub><sup>1</sup>| h2\n",
    "    x5 --> |w<sub>53</sub><sup>1</sup>| h3\n",
    "\n",
    "    h1 --> |w<sub>11</sub><sup>2</sup>| y\n",
    "    h2 --> |w<sub>21</sub><sup>2</sup>| y\n",
    "    h3 --> |w<sub>31</sub><sup>2</sup>| y\n",
    "\n",
    "    y --> Out([\"Softmax Activation\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. One-Hot Encoding Representation**\n",
    "Since we have five unique words {\"Show\", \"is\", \"nice\", \"worst\", \"not\"}, each word is a 5-dimensional vector:\n",
    "\n",
    "| Word  | One-Hot Encoding |\n",
    "|--------|-----------------|\n",
    "| Show  | `[1, 0, 0, 0, 0]` |\n",
    "| is    | `[0, 1, 0, 0, 0]` |\n",
    "| nice   | `[0, 0, 1, 0, 0]` |\n",
    "| worst    | `[0, 0, 0, 1, 0]` |\n",
    "| not    | `[0, 0, 0, 0, 1]` |\n",
    "\n",
    "Each word is now represented as a **5-dimensional vector**.\n",
    "\n",
    "### **4. Defining RNN Parameters**\n",
    "- **Weight matrices:**\n",
    "  - Input-to-Hidden weights $(W_x)$: `3 × 5` matrix\n",
    "  - Hidden-to-Hidden weights $(W_h)$: `3 × 3` matrix\n",
    "  - Bias $(b)$: `3 × 1` vector\n",
    "  - Hidden-to-Output weights $(W_y)$: `1 × 3` matrix\n",
    "  - Output bias $(b_y)$: `1 × 1` scalar\n",
    "  \n",
    "#### **Weight Matrices**\n",
    "$$\n",
    "W_x = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_h = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "b = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_y = \\begin{bmatrix} 0.5 & 0.6 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "b_y = \\begin{bmatrix} 0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **5. Forward Propagation Formula**\n",
    "For each time step `t`:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "$$\n",
    "y_t = \\text{softmax}(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "  - $x_t$ = Input word (one-hot encoded vector of shape `5 × 1`)\n",
    "  - $h_t$ = Hidden state (`3 × 1`)\n",
    "  - $y_t$ = Output (`1 × 1` scalar)\n",
    "\n",
    "### **6. Forward Propagation Calculation**\n",
    "#### **Step 1: Processing First Word \"Show\" (t = 1)**\n",
    "\n",
    "##### **1. Input Vector for \"Show\"**\n",
    "$$\n",
    "x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **2. Detailed calculation of $(W_x \\cdot x_1)$:**\n",
    "$$\n",
    "W_x x_1 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} (0.1 \\times 1) + (0.2 \\times 0) + (0.3 \\times 0) + (0.4 \\times 0) + (0.5 \\times 0) \\\\ (0.6 \\times 1) + (0.7 \\times 0) + (0.8 \\times 0) + (0.9 \\times 0) + (1.0 \\times 0) \\\\ (1.1 \\times 1) + (1.2 \\times 0) + (1.3 \\times 0) + (1.4 \\times 0) + (1.5 \\times 0) \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.1 \\\\ 0.6 \\\\ 1.1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **3. Detailed calculation of $W_h \\cdot h_0$:**\n",
    "\n",
    "Since $h_0$ is initialized to zeros:\n",
    "$$\n",
    "W_h h_0 = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **4. calculate $z_1$:**\n",
    "$$\n",
    "z_1 = W_x x_1 + W_h h_0 + b = \\begin{bmatrix} 0.1 \\\\ 0.6 \\\\ 1.1 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.7 \\\\ 1.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **5. Apply tanh activation**\n",
    "$$\n",
    "h_1 = \\tanh(z_1) = \\begin{bmatrix} \\tanh(0.2) \\\\ \\tanh(0.7) \\\\ \\tanh(1.2) \\end{bmatrix} \\approx \\begin{bmatrix} 0.198 \\\\ 0.604 \\\\ 0.833 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Step 2: Processing Second Word \"is\" (t = 2)**\n",
    "##### **1. Input Vector for \"is\"**\n",
    "$$\n",
    "x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **2. Detailed calculation of $W_x \\cdot x_2$:**\n",
    "$$\n",
    "W_x  x_2 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} (0.1 \\times 0) + (0.2 \\times 1) + (0.3 \\times 0) + (0.4 \\times 0) + (0.5 \\times 0) \\\\ (0.6 \\times 0) + (0.7 \\times 1) + (0.8 \\times 0) + (0.9 \\times 0) + (1.0 \\times 0) \\\\ (1.1 \\times 0) + (1.2 \\times 1) + (1.3 \\times 0) + (1.4 \\times 0) + (1.5 \\times 0) \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.2 \\\\ 0.7 \\\\ 1.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **3. Detailed calculation of $W_h \\cdot h_1$:**\n",
    "$$\n",
    "W_h h_1 = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0.198 \\\\ 0.604 \\\\ 0.833 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} (0.9 \\times 0.198) + (0.8 \\times 0.604) + (0.7 \\times 0.833) \\\\ (0.6 \\times 0.198) + (0.5 \\times 0.604) + (0.4 \\times 0.833) \\\\ (0.3 \\times 0.198) + (0.2 \\times 0.604) + (0.1 \\times 0.833) \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.178 + 0.483 + 0.583 \\\\ 0.119 + 0.302 + 0.333 \\\\ 0.059 + 0.121 + 0.083 \\end{bmatrix} = \\begin{bmatrix} 1.244 \\\\ 0.754 \\\\ 0.263 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **4. calculate $z_2$:**\n",
    "$$\n",
    "z_2 = W_x x_2 + W_h h_1 + b = \\begin{bmatrix} 0.2 \\\\ 0.7 \\\\ 1.2 \\end{bmatrix} + \\begin{bmatrix} 1.244 \\\\ 0.754 \\\\ 0.263 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix} = \\begin{bmatrix} 1.544 \\\\ 1.554 \\\\ 1.563 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **5. Apply tanh activation**\n",
    "$$\n",
    "h_2 = \\tanh(z_2) = \\begin{bmatrix} \\tanh(1.544) \\\\ \\tanh(1.554) \\\\ \\tanh(1.563) \\end{bmatrix} \\approx \\begin{bmatrix} 0.911 \\\\ 0.914 \\\\ 0.917 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Step 3: Processing Third Word \"nice\" (t = 3)**\n",
    "\n",
    "##### **1. Input Vector for \"nice\"**\n",
    "$$\n",
    "x_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **2. Calculate $W_x \\cdot x_3$**\n",
    "$$\n",
    "W_x x_3 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.3 \\\\ 0.8 \\\\ 1.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **3. Calculate $W_h \\cdot h_2$**\n",
    "From **Step 2**, we have:\n",
    "$$\n",
    "h_2 = \\begin{bmatrix} 0.911 \\\\ 0.914 \\\\ 0.917 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now compute:\n",
    "$$\n",
    "W_h h_2 = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0.911 \\\\ 0.914 \\\\ 0.917 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 2.193 \\\\ 1.371 \\\\ 0.548 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **4. Calculate $z_3$**\n",
    "Add the bias:\n",
    "$$\n",
    "z_3 = W_x x_3 + W_h h_2 + b = \\begin{bmatrix} 0.3 \\\\ 0.8 \\\\ 1.3 \\end{bmatrix} + \\begin{bmatrix} 2.193 \\\\ 1.371 \\\\ 0.548 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 2.593 \\\\ 2.271 \\\\ 1.948 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **5. Apply tanh activation**\n",
    "$$\n",
    "h_3 = \\tanh(z_3) = \\begin{bmatrix} \\tanh(2.593) \\\\ \\tanh(2.271) \\\\ \\tanh(1.948) \\end{bmatrix} \\approx \\begin{bmatrix} 0.989 \\\\ 0.979 \\\\ 0.961 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Step 4: Output Calculation for \"nice\"**\n",
    "\n",
    "##### **1. Calculate $W_y \\cdot h_3$**\n",
    "$$\n",
    "W_y h_3 = \\begin{bmatrix} 0.5 & 0.6 & 0.7 \\end{bmatrix} \\begin{bmatrix} 0.989 \\\\ 0.979 \\\\ 0.961 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= 0.495 + 0.587 + 0.673 = 1.755\n",
    "$$\n",
    "\n",
    "##### **2. Add output bias**\n",
    "$$\n",
    "W_y h_3 + b_y = 1.755 + 0.2 = 1.955\n",
    "$$\n",
    "\n",
    "##### **3. Apply softmax/sigmoid activation**\n",
    "$$\n",
    "y_{\\text{final}} = \\frac{1}{1 + e^{-1.955}} \\approx 0.876\n",
    "$$\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "1. The **hidden state** is updated at each time step using the input word and the previous hidden state.\n",
    "2. The output is computed using the final hidden state and passed through a **softmax/sigmoid activation**.\n",
    "3. This process can be repeated for any number of words in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#dcdede', 'fontSize': '25px', 'textWrapWidth': 200 }, 'viewBox': '0 0 1200 1200' }}%%\n",
    "graph LR\n",
    "    subgraph \"t_0\" [\"Time step t=0\"]\n",
    "        direction TB\n",
    "        style t_0 fill:#9199e1,stroke:#999988,stroke-width:2px,font-size:25px,color:#080b2c\n",
    "        \n",
    "        subgraph inputs0 [\"Input: Show\"]\n",
    "            direction LR\n",
    "            style inputs0 fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "            x0[\"x₀ = [1,0,0,0,0]\"]\n",
    "        end\n",
    "        \n",
    "        subgraph hidden0 [\"Hidden Layer\"]\n",
    "            direction LR\n",
    "            style hidden0 fill:#5fb48b,stroke:#85ff9f,stroke-width:2px\n",
    "            h0_1(h_01)\n",
    "            h0_2(h_02)\n",
    "            h0_3(h_03)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    subgraph \"t_1\" [\"Time step t=1\"]\n",
    "        direction TB\n",
    "        style t_1 fill:#e19f91,stroke:#999999,stroke-width:2px,font-size:25px,color:#080b2c\n",
    "        \n",
    "        subgraph inputs1 [\"Input: is\"]\n",
    "            direction LR\n",
    "            style inputs1 fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "            x1[\"x₁ = [0,1,0,0,0]\"]\n",
    "        end\n",
    "        \n",
    "        subgraph hidden1 [\"Hidden Layer\"]\n",
    "            direction LR\n",
    "            style hidden1 fill:#5fb48b,stroke:#85ff9f,stroke-width:2px\n",
    "            h1_1(h_01)\n",
    "            h1_2(h_02)\n",
    "            h1_3(h_03)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    subgraph \"t_2\" [\"Time step t=2\"]\n",
    "        direction TB\n",
    "        style t_2 fill:#c891e1,stroke:#999999,stroke-width:2px,font-size:25px,color:#080b2c\n",
    "        \n",
    "        subgraph inputs2 [\"Input: nice\"]\n",
    "            direction LR\n",
    "            style inputs2 fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "            x2[\"x₂ = [0,0,1,0,0]\"]\n",
    "        end\n",
    "        \n",
    "        subgraph hidden2 [\"Hidden Layer\"]\n",
    "            direction LR\n",
    "            style hidden2 fill:#5fb48b,stroke:#85ff9f,stroke-width:2px\n",
    "            h2_1(h_01)\n",
    "            h2_2(h_02)\n",
    "            h2_3(h_03)\n",
    "        end\n",
    "        \n",
    "        subgraph output [\"Output\"]\n",
    "            direction LR\n",
    "            style output fill:#85929e,stroke:#f8cc52,stroke-width:2px\n",
    "            y_final(y)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    %% Input to hidden connections with Wx\n",
    "    x0 -.->|W_x| h0_1\n",
    "    x0 -.->|W_x| h0_2\n",
    "    x0 -.->|W_x| h0_3\n",
    "    \n",
    "    x1 -.->|W_x| h1_1\n",
    "    x1 -.->|W_x| h1_2\n",
    "    x1 -.->|W_x| h1_3\n",
    "    \n",
    "    x2 -.->|W_x| h2_1\n",
    "    x2 -.->|W_x| h2_2\n",
    "    x2 -.->|W_x| h2_3\n",
    "    \n",
    "    %% Recurrent connections with Wh (Red)\n",
    "    h0_1 ===>|W_h| h1_1\n",
    "    h0_2 ===>|W_h| h1_1\n",
    "    h0_3 ===>|W_h| h1_1\n",
    "    \n",
    "    h0_1 ===>|W_h| h1_2\n",
    "    h0_2 ===>|W_h| h1_2\n",
    "    h0_3 ===>|W_h| h1_2\n",
    "    \n",
    "    h0_1 ===>|W_h| h1_3\n",
    "    h0_2 ===>|W_h| h1_3\n",
    "    h0_3 ===>|W_h| h1_3\n",
    "    \n",
    "    h1_1 ===>|W_h| h2_1\n",
    "    h1_2 ===>|W_h| h2_1\n",
    "    h1_3 ===>|W_h| h2_1\n",
    "    \n",
    "    h1_1 ===>|W_h| h2_2\n",
    "    h1_2 ===>|W_h| h2_2\n",
    "    h1_3 ===>|W_h| h2_2\n",
    "    \n",
    "    h1_1 ===>|W_h| h2_3\n",
    "    h1_2 ===>|W_h| h2_3\n",
    "    h1_3 ===>|W_h| h2_3\n",
    "    \n",
    "    %% Hidden to output connections (Red)\n",
    "    h2_1 ==> |W_y| y_final\n",
    "    h2_2 ==> |W_y| y_final\n",
    "    h2_3 ==> |W_y| y_final\n",
    "    \n",
    "    y_final --> Out([\"Softmax Activation\"])\n",
    "    \n",
    "    %% Bias connections are implied\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (None, 3)                 27        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31 (124.00 Byte)\n",
      "Trainable params: 31 (124.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "model1 = Sequential()\n",
    "model1.add(SimpleRNN(3, input_shape=(None, 5), activation='tanh'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "# calculating total trainable parameters\n",
    "(5*3+3)+(3*3)+(3*1+1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
